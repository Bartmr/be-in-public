{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Framing\n",
    "\n",
    "**Labels** are usually represented by the variable $ y $ \\\n",
    "**Features** are typically represented by the variables $ x_{1} $, $ x_{2} $, ..., $ x_{n} $\n",
    "\n",
    "### $ L_{2} $ Loss / squared loss\n",
    "\n",
    "$ L_{2} = (\\text{observation} - \\text{prediction})^{2} $\n",
    "\n",
    "## Linear Regression\n",
    "\n",
    "$ y' = b + w_{1}x_{1} $ \\\n",
    "$ y' $ is the label \\\n",
    "$ b $ is the bias (the y-intercept), sometimes referred to as $ w_{0} $ \\\n",
    "$ w_{1} $ is the weight of feature 1. Weight is the same concept as the \"slope\"  in the traditional equation of a line. \\\n",
    "$ x_{1} $ is a feature\n",
    "\n",
    "Although this model uses only one feature, a more sophisticated model might rely on multiple features, each having a separate weight ($ x_{1} $, $ x_{2} $, etc.). For example, a model that relies on three features might look as follows:\n",
    "\n",
    "$ y' = b + w_{1}x_{1} + w_{2}x_{2} + w_{3}x_{3} $\n",
    "\n",
    "## Loss\n",
    "\n",
    "**[Mean square error](https://developers.google.com/machine-learning/crash-course/descending-into-ml/training-and-loss#squared-loss:-a-popular-loss-function)**\n",
    "\n",
    "$ \\text{MSE} = \\frac{1}{N} \\sum\\limits_{(x, y) \\in D} ( y - \\text{prediction}(x) )^2 $ \\\n",
    "\n",
    "- $ N $ is the number of examples in $ D $ \\\n",
    "- $ D $ is a data set containing many labeled examples, which are $ (x, y) $ pairs. \\\n",
    "- $ \\sum\\limits_{(x, y) \\in D} $ is a sum limited by\n",
    "  - $ x $ is the set of features that the model uses to make predictions\n",
    "  - $ y $ is the label\n",
    "  - $ \\in D $ must be in the dataset\n",
    "\n",
    "### Derivatives\n",
    "\n",
    "Function that represents the current slope of a line or curve in a given point.\n",
    "\n",
    "#### Prime Notation\n",
    "\n",
    "**y-prime is expressed** as $ y' $ \\\n",
    "\n",
    "$ f´(x) $ is the derivative of function $ f(x) $ which means (for every x) the slope of the function f or $ \\frac{\\Delta f(x)}{\\Delta x} $.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- $ f(x)=a +  m*x $ is a stright line with slope $ m $ then $ f´(x)=m $\n",
    "- the parabolic $ f(x)= x^{2} $ has a slope of $ f´(x)= 2*x $\n",
    "\n",
    "### Gradient Descend\n",
    "\n",
    "As noted, the gradient vector has both a direction and a magnitude. Gradient descent algorithms multiply the gradient by a scalar known as the **learning rate (also sometimes called step size)** to determine the next point. For example, if the gradient magnitude is 2.5 and the learning rate is 0.01, then the gradient descent algorithm will pick the next point 0.025 away from the previous point.\n",
    "\n",
    "Hyperparameters are the knobs that programmers tweak in machine learning algorithms. Most machine learning programmers spend a fair amount of time tuning the learning rate. **If you pick a learning rate that is too small, learning will take too long.**\n",
    "\n",
    "Conversely, if you specify a learning rate that is too large, the next point will perpetually bounce haphazardly across the bottom (which is where minimum loss is).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
