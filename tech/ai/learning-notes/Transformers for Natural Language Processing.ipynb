{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<https://github.com/Denis2054/Transformers-for-NLP-2nd-Edition>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Markov Decision Process (MDP) / Markov Chains / Markov Processes\n",
    "- RNN Recurrent neural network\n",
    "- CNN Convolutional Neural Network\n",
    "\n",
    "---\n",
    "\n",
    "- **RNN formula**\n",
    "  \n",
    "  Each state S n captures the information of S n-1 . When the networkâ€™s end is reached, a function F will perform an action: transduction, modeling, or any other type of sequence-based task.\n",
    "\n",
    "  $ s_{0} $ -> $ s_{1} = h(s_{-1}) $ --> $ s_{n} = h(s_{-1}) $ -> $ F $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<https://demo.allennlp.org/coreference-resolution>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Transformer** architecture: https://en.wikipedia.org/wiki/Transformer_(machine_learning_model)\n",
    "\n",
    "_\"Attention is all you need\"_ paper from Google in 2017. The transformer architecture allows to paralelise work instead of doing it in sequence.\n",
    "\n",
    "The Transformer architecture enables models to process text in a bidirectional manner, from start to finish and from finish to start. This has been central to the limits of previous models which could only process text from start to finish. (from <https://neptune.ai/blog/bert-and-the-transformer-architecture>)\n",
    "\n",
    "https://towardsdatascience.com/attention-is-all-you-need-e498378552f9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://pytorch.org/tutorials/beginner/transformer_tutorial.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
